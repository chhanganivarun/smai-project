{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DsP1KYOvBua",
    "outputId": "327141e5-8812-4170-b3a9-a57684bf679a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bcolz in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from bcolz) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "!pip install bcolz\n",
    "import bcolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZINtfyTvHG8"
   },
   "outputs": [],
   "source": [
    "! tar -xvf drive/MyDrive/Flickr-8K.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "lIcT06tsm3Nh",
    "outputId": "511644d8-4310-4528-c36c-9efc306abdf6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'3.2.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jc4XhapKvju5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "spacy_eng = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RE4hxLAd236P"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_thresh):\n",
    "        self.freq_thresh = freq_thresh\n",
    "        self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                if frequencies[word] == self.freq_thresh:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        tok_text = self.tokenizer(text)\n",
    "        vec = [self.stoi[word] if word in self.stoi else self.stoi[\"<UNK>\"] for word in tok_text]\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CvdDuR35Q5f"
   },
   "outputs": [],
   "source": [
    "# for test and train we need to create new text files of the format (Flickr8k.token.txt)\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, caption_file, img_file, freq_thresh, transform=None,):\n",
    "        self.freq_thresh = freq_thresh\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.img_file = img_file\n",
    "        self.caption_file = caption_file\n",
    "        \n",
    "        self.caption_dict = self.imgId_caption_dict()\n",
    "        self.imgs , self.captions = self.load_img_caption()\n",
    "        self.vocab = Vocabulary(self.freq_thresh)\n",
    "        self.vocab.build_vocab(self.captions)\n",
    "\n",
    "    def imgId_caption_dict(self):\n",
    "        caption_dict = {}\n",
    "        with open(self.caption_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                temp = line.split()\n",
    "                img_name, _ = temp[0].split('#')  # first word will be img_id\n",
    "                description = \"\".join(temp[1:]) # get back the description\n",
    "                if img_name not in caption_dict:\n",
    "                    caption_dict[img_name] = [description]\n",
    "                else:\n",
    "                    caption_dict[img_name].append(description)\n",
    "        return caption_dict\n",
    "\n",
    "    def load_img_caption(self):\n",
    "        imgs = []\n",
    "        captions = []\n",
    "        with open(self.img_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                for caption in self.caption_dict[line]:\n",
    "                    imgs.append(line)\n",
    "                    captions.append(caption)\n",
    "        return imgs, captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption += [self.vocab.stoi[\"<EOS>\"]]\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zpNAXTVPH08"
   },
   "outputs": [],
   "source": [
    "# we can also define simply a function instead of a class\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0) # batch * imgs_size * 3 (RGB)\n",
    "        captions = [item[1] for item in batch] \n",
    "        captions = pad_sequence(captions, batch_first=False, padding_value=self.pad_idx)\n",
    "        return imgs, captions # return the batched images and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjF4o2BTQ7is"
   },
   "outputs": [],
   "source": [
    "def get_loader(root_dir, caption_file, img_file, transform, batch=32, num_worker=2, shuffle=True, pin_memory=True):\n",
    "    dataset = FlickrDataset(root_dir=root_dir, caption_file=caption_file, img_file=img_file, freq_thresh=5, transform=transform)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch,\n",
    "                        shuffle=shuffle,\n",
    "                        collate_fn=MyCollate(pad_idx),\n",
    "                        pin_memory=pin_memory,\n",
    "                        num_workers=num_worker)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1JT-_sOStDM"
   },
   "outputs": [],
   "source": [
    "# Need to write a transform for images.\n",
    "trans = transforms.Compose([\n",
    "                            transforms.Resize((224, 224)),\n",
    "                            transforms.ToTensor(),\n",
    "])\n",
    "dataloader = get_loader(root_dir=\"Flickr-8K/Flicker8k_Dataset\",\n",
    "                        img_file=\"Flickr-8K/Flickr_8k.testImages.txt\",\n",
    "                        caption_file=\"Flickr-8K/Flickr8k.token.txt\",\n",
    "                        transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vlCx4D0Mbvq"
   },
   "outputs": [],
   "source": [
    "for idx, (img, caption) in enumerate(dataloader):\n",
    "    print(img.shape)\n",
    "    print(caption.shape)\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxlkHtg82H9h"
   },
   "source": [
    "## Embedding layer using GLoVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7aRdnFzSQpX",
    "outputId": "9161d115-a0bd-423c-d9e8-bda4913d5ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  drive/MyDrive/glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "! unzip drive/MyDrive/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ts49sZeclo0E"
   },
   "outputs": [],
   "source": [
    "# Need not run this again and again\n",
    "\n",
    "\n",
    "# words = []\n",
    "# idx = 0\n",
    "# word2idx = {}\n",
    "# vectors = bcolz.carray(np.zeros(1), rootdir=f'drive/MyDrive/6B.300.dat', mode='w')\n",
    "\n",
    "# with open(f'glove.6B.300d.txt', 'rb') as f:\n",
    "#     for l in f:\n",
    "#         line = l.decode().split()\n",
    "#         word = line[0]\n",
    "#         words.append(word)\n",
    "#         word2idx[word] = idx\n",
    "#         idx += 1\n",
    "#         vect = np.array(line[1:]).astype(np.float)\n",
    "#         vectors.append(vect)\n",
    "    \n",
    "# vectors = bcolz.carray(vectors[1:].reshape((400000, 300)), rootdir=f'drive/MyDrive/6B.300.dat', mode='w')\n",
    "# vectors.flush()\n",
    "# pickle.dump(words, open(f'drive/MyDrive/6B.300_words.pkl', 'wb'))\n",
    "# pickle.dump(word2idx, open(f'drive/MyDrive/6B.300_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTC8FSOzl9QE"
   },
   "outputs": [],
   "source": [
    "vectors = bcolz.open(f'drive/MyDrive/6B.300.dat')[:]\n",
    "words = pickle.load(open(f'drive/MyDrive/6B.300_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'drive/MyDrive/6B.300_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDviwja199ni",
    "outputId": "bcf99064-6815-4233-a28f-aeabcb0885d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['the'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WneZPkxJPFN"
   },
   "outputs": [],
   "source": [
    "# Get the flickr Dataset and access the vocab from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "Tykrx1ZrsHfs",
    "outputId": "f0ff53bb-93ff-4e92-8fa4-c0d02c8e827e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fff9dca5a6e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatrix_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_vocab\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# target vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mweights_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwords_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "matrix_len = len(target_vocab)  # target vocabulary \n",
    "weights_matrix = np.zeros((matrix_len, 300))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(target_vocab):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(300, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxiiDsjBtKJK"
   },
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_mwuney4bOQ"
   },
   "source": [
    "## Code for Encoder CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TYXCC3stdBX"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = models.resnet152(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = self.model(X)\n",
    "        return self.dropout(self.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCiYUhkfQSLm"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    takes as input img vector and captions\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self, vocab)\n",
    "        self.input_size = 512  # m = input_size in paper\n",
    "        self.hidden_size = 512 # n = hidden_size in paper\n",
    "        self.word_size = 300\n",
    "        self.image_size = 1024\n",
    "        self.lstm = nn.LSTMCell(self.input_size, self.hidden_size) # using single cell rather than lstm\n",
    "        self.U = nn.Linear(self.word_size, self.word_size)\n",
    "        self.V = nn.Linear(self.hidden_size, self.word_size)\n",
    "        self.W_Y_A = nn.Linear(self.word_size, self.hidden_size)\n",
    "        self.W_x_Y = nn.Linear(self.word_size, self.input_size)\n",
    "        self.W_Y_h = nn.Linear(self.hidden_size, self.word_size)\n",
    "        self.image_mapping = nn.Linear(self.image_size, self.input_size)\n",
    "        self.diag_weight = torch.randn(self.word_size, requires_grad=True)\n",
    "        self.input_diag_mat = torch.diag(self.diag_weight)\n",
    "\n",
    "        # use the pretrained glove embedding\n",
    "        self.vocab, self.num_embedding = create_emb_layer(weight_matrix, True)\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.ReLU = nn.ReLU(inplace = True)\n",
    "\n",
    "    def input_attention(self, previous_word, attributes):\n",
    "        \"\"\" \n",
    "        Apply input attention at each step.\n",
    "        attributes -> matrix where columns store the word embedding\n",
    "        \"\"\"\n",
    "        # for each word in the vocabulary find the score\n",
    "        previous_word = torch.unsqueeze(previous_word, 0) # batch x 300\n",
    "        score = torch.mm(self.U(previous_word), attributes) # batch x no_of_attributes\n",
    "        score = self.soft(score)  # batch x no_of_attributes\n",
    "        score = torch.transpose(score, 0, 1) # no_of_attributes x batch\n",
    "        weighted_y = torch.mm(attributes, score)  # 300 x batch\n",
    "        scaled_y = torch.mm(self.input_diag_mat, weighted_y)  # 300 x batch\n",
    "        final_y = scaled_y + torch.transpose(previous_word, 0, 1) # 300 x batch\n",
    "        x = self.W_x_Y(torch.transpose(final_y, 0, 1))  # batch x 512\n",
    "        return x, torch.transpose(score, 0, 1)\n",
    "\n",
    "    def output_attention(self, hidden_state, attributes):\n",
    "        \"\"\"\n",
    "        Apply output attention at each step\n",
    "        hidden_state = current hidden state of LSTMCell, used for predicting current output\n",
    "        attributes -> matrix where columns store the word embedding\n",
    "        \"\"\"\n",
    "        hidden_state = torch.unsqueeze(hidden_state, 0) # batch x 512\n",
    "        score = torch.mm(self.V(hidden_state), self.tanh(attributes)) # batch x no_of_attributes\n",
    "        score = self.soft(score)  # batch x no_of_attributes\n",
    "        score = torch.transpose(score, 0, 1) # no_of_attributes x batch\n",
    "        weighted_y = torch.mm(self.tanh(attributes), score) # 300 x batch\n",
    "        y_to_h = self.W_Y_A(torch.transpose(weighted_y, 0, 1))  # batch x 512\n",
    "        hidden = hidden_state + y_to_h  # batch x 512\n",
    "        hidden = self.W_Y_h(hidden)  # batch x 300\n",
    "        logits = torch.mm(torch.transpose(attributes, 0, 1), torch.transpose(hidden, 0, 1)) # no_of_attr x batch\n",
    "        logits = torch.transpose(logits, 0, 1) # batch x no_of_attr\n",
    "        return logits, torch.transpose(score, 0, 1)  \n",
    "\n",
    "    def prepare_attributes(self, attr):\n",
    "        no_of_attr = attr.shape[1] # batch x no_of_attr\n",
    "        batch = attr.shape[0] \n",
    "        attributes = torch.zeros(batch, no_of_attr, 300)\n",
    "        for i in range(batch):\n",
    "            for j in range(no_of_attr):\n",
    "                attributes[i, j, :] = self.vocab[attr[i, j]]\n",
    "        return attributes\n",
    "\n",
    "    def next_word(self, logits):\n",
    "        \"\"\"\n",
    "        Samples from the top 3 words randomly and returns a word\n",
    "        \"\"\"\n",
    "        probability = torch.softmax(logits, dim=1)  # batch x no_of_attr\n",
    "        prob_sort, indices = torch.sort(probability, descending=True)\n",
    "        # generate a array of size -> batch\n",
    "        arr = torch.randint(3, (logits.shape[0],))\n",
    "        random_words = torch.zeros(logits.shape[0], 300)\n",
    "\n",
    "        for i in range(logits.shape[0]):\n",
    "            ind = inidices[i, arr[i]]\n",
    "            random_words[i] = self.vocab[ind]\n",
    "        return random_words  # batch x 300\n",
    "\n",
    "    def forward(self, seq_len, image_vectors, attributes, caption_ids=None):\n",
    "        \"\"\"\n",
    "        image_vectors -> batch x img_size\n",
    "        caption_ids -> batch x seq_len x word_size (make it not None, if you want to do teacher forcing)\n",
    "        attributes -> batch x 20, we have 20 attributes for each of the image (contains indices of words in our dictionary)\n",
    "        it should also contain <PAD>, <SOS>, <EOS> as well. \n",
    "        \"\"\"\n",
    "        image_vectors = self.ReLU(self.image_mapping(image_vectors)) # batch x 512\n",
    "        attributes = self.prepare_attributes(attributes)\n",
    "        batch_size = image_vectors.shape[0]\n",
    "\n",
    "        Hidden_logits = torch.zeros(batch_size, seq_len, attributes.shape[1])  # batch x seq x no_of_attr\n",
    "        Hidden_scores = torch.zeros(batch_size, seq_len, attributes.shape[1]) # batch x seq x no_of_attr\n",
    "        Input_scores = torch.zeros(batch_size, seq_len, attributes.shape[1]) # batch x seq x no_of_attr\n",
    "\n",
    "        hidden, cell = self.lstm(image_vectors) # batch x 512\n",
    "        # Hidden_states.append(hidden.unsqueeze(1))    no need to append this first one\n",
    "        logits, hidden_score = self.output_attention(hidden, attributes)\n",
    "        word = self.next_word(logits)  # batch x 300\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            # prepare the input using input attention\n",
    "            next_input, input_score = self.input_attention(word, attributes)\n",
    "            Input_scores[:, i, :] = input_score\n",
    "            hidden, cell = self.lstm(next_input, (hidden, cell))\n",
    "            logits, hidden_score = self.output_attention(hidden, attributes)\n",
    "            Hidden_scores[:, i, :] = hidden_score\n",
    "            Hidden_logits[:, i, :] = logits\n",
    "            word = self.next_word(logits)  # batch x 300\n",
    "\n",
    "        return Hidden_logits, Hidden_scores, Input_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFgZqLv3WJmK"
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "class MyLoss(nn.Module):\n",
    "    \"\"\"Crossentropy + regularization\"\"\"\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, output, target, Hidden_scores, Input_scores):\n",
    "        # output -> batch x seq x no_of_attr\n",
    "        # target -> batch x seq\n",
    "        x = self.criterion(output, target)  # batch x seq_len\n",
    "        x = x.item()\n",
    "        x = torch.sum(x, dim=1)  # batch\n",
    "        hidden_reg = self.regularize(Hidden_scores)\n",
    "        input_reg = self.regularize(Input_scores)\n",
    "        return x + hidden_reg + input_reg\n",
    "\n",
    "    def regularize(self, scores):\n",
    "        # batch x time x i(attr index)\n",
    "        s1 = torch.sum(score, dim=1)\n",
    "        s1 = s1 * s1\n",
    "        s1 = torch.sqrt(torch.sum(s1, dim=1)) # sum_i_()^{0.5}\n",
    "\n",
    "        s2 = torch.sqrt(score)\n",
    "        s2 = torch.sum(s2, dim=2)\n",
    "        s2 = s2 * s2\n",
    "        s2 = torch.sum(s2, dim=1)  # sum_time_()^{2}\n",
    "        return s1 + s2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dh543GsHrw6g"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQTjGgWcrjFt"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "criterion = MyLoss()\n",
    "encoder = EncoderCNN(1024)\n",
    "decoder = DecoderRNN()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for idx, (img, caption) in enumerate(dataloader):\n",
    "        # write training loop\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "smaiProject.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
